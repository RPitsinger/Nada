{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": "## Physics-Informed Neural Network for 1D Heat Equation in PyTorch\n\nThis notebook implements a Physics-Informed Neural Network (PINN) to solve the 1D heat equation using PyTorch.\n\n### Problem Statement\n\nWe want to solve the 1D heat equation:\n\n$$ \\frac{\\partial u}{\\partial t}(x,t) = \\alpha \\frac{\\partial^2 u}{\\partial x^2}(x,t)$$\n\nfor $x\\in [0,L]$ and $t>0$ with:\n\n**Initial condition:**\n$$ u(x,0) = \\sin(\\pi x/L) $$\n\n**Boundary conditions:**\n$$ u(0,t) = 0, \\quad u(L,t) = 1$$\n\nThe physics-informed neural network approach minimizes the residual of the PDE:\n\n$$ D_u(x,t) = \\frac{\\partial u}{\\partial t} - \\alpha \\frac{\\partial^2 u}{\\partial x^2} = 0 $$"
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyDOE import lhs\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "params-header",
   "metadata": {},
   "source": [
    "## Problem Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "params",
   "metadata": {},
   "outputs": [],
   "source": "# Domain parameters\nL = 1.0          # Length of spatial domain\nT = 10.0         # Maximum time\nalpha = 0.01     # Thermal diffusivity\n\n# Domain bounds\nx_min, x_max = 0.0, L\nt_min, t_max = 0.0, T\n\nprint(f'Spatial domain: [{x_min}, {x_max}]')\nprint(f'Time domain: [{t_min}, {t_max}]')\nprint(f'Thermal diffusivity: {alpha}')"
  },
  {
   "cell_type": "markdown",
   "id": "network-header",
   "metadata": {},
   "source": [
    "## Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "network",
   "metadata": {},
   "outputs": [],
   "source": "class PINN(nn.Module):\n    \"\"\"Physics-Informed Neural Network for the Heat Equation\"\"\"\n    \n    def __init__(self, layers):\n        super(PINN, self).__init__()\n        \n        # Network architecture\n        self.layers = layers\n        self.num_layers = len(layers)\n        \n        # Create network layers\n        self.linears = nn.ModuleList([\n            nn.Linear(layers[i], layers[i+1]) for i in range(self.num_layers - 1)\n        ])\n        \n        # Initialize weights using Xavier initialization\n        for i in range(self.num_layers - 1):\n            nn.init.xavier_normal_(self.linears[i].weight)\n            nn.init.zeros_(self.linears[i].bias)\n    \n    def forward(self, x, t):\n        \"\"\"Forward pass through the network\n        \n        Args:\n            x: spatial coordinates (batch_size, 1)\n            t: temporal coordinates (batch_size, 1)\n        \n        Returns:\n            u: predicted temperature (batch_size, 1)\n        \"\"\"\n        # Concatenate inputs\n        inputs = torch.cat([x, t], dim=1)\n        \n        # Forward pass through hidden layers with tanh activation\n        h = inputs\n        for i in range(self.num_layers - 2):\n            h = torch.tanh(self.linears[i](h))\n        \n        # Output layer (linear)\n        u = self.linears[-1](h)\n        return u\n    \n    def pde_residual(self, x, t, alpha):\n        \"\"\"Compute the PDE residual D_u = u_t - alpha * u_xx\n        \n        Args:\n            x: spatial coordinates\n            t: temporal coordinates\n            alpha: thermal diffusivity\n        \n        Returns:\n            residual: PDE residual\n        \"\"\"\n        # Enable gradient computation\n        x.requires_grad_(True)\n        t.requires_grad_(True)\n        \n        # Compute u\n        u = self.forward(x, t)\n        \n        # First derivatives\n        u_t = torch.autograd.grad(\n            u, t, \n            grad_outputs=torch.ones_like(u),\n            retain_graph=True,\n            create_graph=True\n        )[0]\n        \n        u_x = torch.autograd.grad(\n            u, x,\n            grad_outputs=torch.ones_like(u),\n            retain_graph=True,\n            create_graph=True\n        )[0]\n        \n        # Second derivative\n        u_xx = torch.autograd.grad(\n            u_x, x,\n            grad_outputs=torch.ones_like(u_x),\n            retain_graph=True,\n            create_graph=True\n        )[0]\n        \n        # PDE residual: u_t - alpha * u_xx = 0\n        residual = u_t - alpha * u_xx\n        \n        return residual\n\n# Define network architecture\nlayers = [2, 32, 32, 32, 1]  # [input_dim, hidden1, hidden2, hidden3, output_dim]\nmodel = PINN(layers).to(device)\n\nprint(f'Network architecture: {layers}')\nprint(f'Total parameters: {sum(p.numel() for p in model.parameters())}')\nprint(model)"
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data",
   "metadata": {},
   "outputs": [],
   "source": "# Number of training points\nN_ic = 100      # Initial condition points\nN_bc = 100      # Boundary condition points\nN_pde = 10000   # Collocation points for PDE residual\n\n# Initial condition: u(x, 0) = sin(pi * x / L)\nx_ic = np.random.uniform(x_min, x_max, (N_ic, 1))\nt_ic = np.zeros((N_ic, 1))\nu_ic = np.sin(np.pi * x_ic / L)\n\n# Boundary condition at x = 0: u(0, t) = 0\nx_bc1 = np.zeros((N_bc, 1))\nt_bc1 = np.random.uniform(t_min, t_max, (N_bc, 1))\nu_bc1 = np.zeros((N_bc, 1))\n\n# Boundary condition at x = L: u(L, t) = 1\nx_bc2 = L * np.ones((N_bc, 1))\nt_bc2 = np.random.uniform(t_min, t_max, (N_bc, 1))\nu_bc2 = np.ones((N_bc, 1))\n\n# Combine all initial and boundary conditions\nx_data = np.vstack([x_ic, x_bc1, x_bc2])\nt_data = np.vstack([t_ic, t_bc1, t_bc2])\nu_data = np.vstack([u_ic, u_bc1, u_bc2])\n\n# Collocation points for PDE residual (Latin Hypercube Sampling)\nlb = np.array([x_min, t_min])\nub = np.array([x_max, t_max])\nX_pde = lb + (ub - lb) * lhs(2, N_pde)\nx_pde = X_pde[:, 0:1]\nt_pde = X_pde[:, 1:2]\n\n# Convert to PyTorch tensors\nx_data_tensor = torch.tensor(x_data, dtype=torch.float32, requires_grad=False).to(device)\nt_data_tensor = torch.tensor(t_data, dtype=torch.float32, requires_grad=False).to(device)\nu_data_tensor = torch.tensor(u_data, dtype=torch.float32, requires_grad=False).to(device)\n\nx_pde_tensor = torch.tensor(x_pde, dtype=torch.float32, requires_grad=True).to(device)\nt_pde_tensor = torch.tensor(t_pde, dtype=torch.float32, requires_grad=True).to(device)\n\nprint(f'Initial condition points: {N_ic}')\nprint(f'Boundary condition points: {2 * N_bc}')\nprint(f'PDE collocation points: {N_pde}')\nprint(f'Total data points: {x_data.shape[0]}')"
  },
  {
   "cell_type": "markdown",
   "id": "viz-data-header",
   "metadata": {},
   "source": [
    "## Visualize Training Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(t_data, x_data, c='blue', s=10, alpha=0.6, label='IC/BC points')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('x')\n",
    "plt.title('Initial and Boundary Condition Points')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(t_pde, x_pde, c='red', s=1, alpha=0.3, label='Collocation points')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('x')\n",
    "plt.title('PDE Collocation Points')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-header",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train",
   "metadata": {},
   "outputs": [],
   "source": "# Training parameters\nepochs = 10000\nlearning_rate = 1e-3\n\n# Optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Loss history\nloss_history = []\nloss_data_history = []\nloss_pde_history = []\n\n# Training loop\nfor epoch in range(epochs):\n    # Zero gradients\n    optimizer.zero_grad()\n    \n    # Predict u at data points\n    u_pred = model(x_data_tensor, t_data_tensor)\n    \n    # Data loss (MSE for initial and boundary conditions)\n    loss_data = torch.mean((u_pred - u_data_tensor) ** 2)\n    \n    # PDE residual loss\n    residual = model.pde_residual(x_pde_tensor, t_pde_tensor, alpha)\n    loss_pde = torch.mean(residual ** 2)\n    \n    # Total loss\n    loss = loss_data + loss_pde\n    \n    # Backpropagation\n    loss.backward()\n    optimizer.step()\n    \n    # Store losses\n    loss_history.append(loss.item())\n    loss_data_history.append(loss_data.item())\n    loss_pde_history.append(loss_pde.item())\n    \n    # Print progress\n    if (epoch + 1) % 1000 == 0:\n        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.6f}, '\n              f'Data Loss: {loss_data.item():.6f}, PDE Loss: {loss_pde.item():.6f}')\n\nprint('\\nTraining completed!')"
  },
  {
   "cell_type": "markdown",
   "id": "loss-viz-header",
   "metadata": {},
   "source": [
    "## Visualize Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loss-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.semilogy(loss_history, 'b-', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Total Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.semilogy(loss_data_history, 'g-', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Data Loss')\n",
    "plt.title('Data Loss (IC/BC)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.semilogy(loss_pde_history, 'r-', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('PDE Loss')\n",
    "plt.title('PDE Residual Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "predict-header",
   "metadata": {},
   "source": [
    "## Prediction and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "predict",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid for prediction\n",
    "N_x = 256\n",
    "N_t = 100\n",
    "\n",
    "x_test = np.linspace(x_min, x_max, N_x)\n",
    "t_test = np.linspace(t_min, t_max, N_t)\n",
    "X_grid, T_grid = np.meshgrid(x_test, t_test)\n",
    "\n",
    "x_flat = X_grid.flatten()[:, None]\n",
    "t_flat = T_grid.flatten()[:, None]\n",
    "\n",
    "# Convert to tensors\n",
    "x_test_tensor = torch.tensor(x_flat, dtype=torch.float32).to(device)\n",
    "t_test_tensor = torch.tensor(t_flat, dtype=torch.float32).to(device)\n",
    "\n",
    "# Predict\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    u_pred = model(x_test_tensor, t_test_tensor)\n",
    "\n",
    "# Reshape for plotting\n",
    "u_pred_grid = u_pred.cpu().numpy().reshape(N_t, N_x)\n",
    "\n",
    "print('Prediction completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analytical-header",
   "metadata": {},
   "source": "## PINN Solution Visualization\n\n**Note:** With the non-homogeneous boundary condition u(L,t) = 1, there is no simple closed-form analytical solution. The PINN learns the solution directly from the physics (PDE) and boundary/initial conditions."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analytical",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize the PINN prediction\nplt.figure(figsize=(10, 6))\nim = plt.imshow(u_pred_grid.T, interpolation='nearest', cmap='hot',\n                extent=[t_min, t_max, x_min, x_max],\n                origin='lower', aspect='auto')\nplt.colorbar(im, label='u(x, t)')\nplt.xlabel('t')\nplt.ylabel('x')\nplt.title('PINN Solution: 1D Heat Equation')\nplt.scatter(t_data, x_data, c='cyan', s=2, alpha=0.5, label='Training points (IC/BC)')\nplt.legend()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "final-viz-header",
   "metadata": {},
   "source": "## Detailed Analysis of PINN Prediction"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-viz",
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# PINN prediction\nax = axes[0, 0]\nim1 = ax.imshow(u_pred_grid.T, interpolation='nearest', cmap='hot',\n                extent=[t_min, t_max, x_min, x_max],\n                origin='lower', aspect='auto')\nax.set_xlabel('t')\nax.set_ylabel('x')\nax.set_title('PINN Solution')\nplt.colorbar(im1, ax=ax)\nax.scatter(t_data, x_data, c='cyan', s=2, alpha=0.5, label='Training points')\nax.legend(loc='upper right')\n\n# Temporal evolution at x=0.25\nax = axes[0, 1]\nx_idx_1 = int(0.25 * N_x / L)\nax.plot(t_test, u_pred_grid[:, x_idx_1], 'b-', linewidth=2, label='x = 0.25')\nx_idx_2 = int(0.5 * N_x / L)\nax.plot(t_test, u_pred_grid[:, x_idx_2], 'g-', linewidth=2, label='x = 0.5')\nx_idx_3 = int(0.75 * N_x / L)\nax.plot(t_test, u_pred_grid[:, x_idx_3], 'r-', linewidth=2, label='x = 0.75')\nax.set_xlabel('t')\nax.set_ylabel('u(x, t)')\nax.set_title('Temporal Evolution at Different x Locations')\nax.legend()\nax.grid(True)\n\n# Spatial profiles at different times\nax = axes[1, 0]\ntime_indices = [0, N_t // 4, N_t // 2, 3 * N_t // 4, N_t - 1]\ncolors = ['blue', 'green', 'orange', 'red', 'purple']\n\nfor idx, color in zip(time_indices, colors):\n    t_val = t_test[idx]\n    ax.plot(x_test, u_pred_grid[idx, :], '-', color=color, \n            linewidth=2, label=f't={t_val:.2f}')\n\nax.set_xlabel('x')\nax.set_ylabel('u(x, t)')\nax.set_title('Spatial Profiles at Different Times')\nax.legend()\nax.grid(True)\n\n# Check boundary conditions\nax = axes[1, 1]\n# Left boundary u(0, t) should be 0\nax.plot(t_test, u_pred_grid[:, 0], 'b-', linewidth=2, label='u(0, t) [should be 0]')\n# Right boundary u(L, t) should be 1\nax.plot(t_test, u_pred_grid[:, -1], 'r-', linewidth=2, label='u(L, t) [should be 1]')\nax.axhline(y=0, color='b', linestyle='--', alpha=0.5)\nax.axhline(y=1, color='r', linestyle='--', alpha=0.5)\nax.set_xlabel('t')\nax.set_ylabel('Boundary values')\nax.set_title('Boundary Condition Satisfaction')\nax.legend()\nax.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# Print boundary condition errors\nbc_error_left = np.mean(np.abs(u_pred_grid[:, 0] - 0))\nbc_error_right = np.mean(np.abs(u_pred_grid[:, -1] - 1))\nprint(f'Mean absolute error at x=0 (should be 0): {bc_error_left:.6e}')\nprint(f'Mean absolute error at x=L (should be 1): {bc_error_right:.6e}')"
  },
  {
   "cell_type": "markdown",
   "id": "residual-header",
   "metadata": {},
   "source": [
    "## Visualize PDE Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residual",
   "metadata": {},
   "outputs": [],
   "source": "# Compute PDE residual on the test grid\nx_test_tensor_grad = torch.tensor(x_flat, dtype=torch.float32, requires_grad=True).to(device)\nt_test_tensor_grad = torch.tensor(t_flat, dtype=torch.float32, requires_grad=True).to(device)\n\nresidual_test = model.pde_residual(x_test_tensor_grad, t_test_tensor_grad, alpha)\nresidual_grid = residual_test.detach().cpu().numpy().reshape(N_t, N_x)\n\nplt.figure(figsize=(10, 6))\nim = plt.imshow(residual_grid.T ** 2, interpolation='nearest', cmap='viridis',\n                extent=[t_min, t_max, x_min, x_max],\n                origin='lower', aspect='auto')\nplt.colorbar(im, label='Squared Residual')\nplt.xlabel('t')\nplt.ylabel('x')\nplt.title('PDE Residual² (Should be close to 0)')\nplt.show()\n\nprint(f'Mean squared residual: {np.mean(residual_grid ** 2):.6e}')"
  },
  {
   "cell_type": "markdown",
   "id": "animation-header",
   "metadata": {},
   "source": [
    "## Create Animation of the Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animation",
   "metadata": {},
   "outputs": [],
   "source": "from matplotlib.animation import FuncAnimation\nfrom IPython.display import HTML\n\n# Create animation\nfig, ax = plt.subplots(figsize=(10, 6))\n\nline_pinn, = ax.plot([], [], 'b-', linewidth=2, label='PINN Solution')\n\nax.set_xlim(x_min, x_max)\nax.set_ylim(-0.1, 1.1)\nax.set_xlabel('x')\nax.set_ylabel('u(x, t)')\nax.set_title('1D Heat Equation Solution')\nax.legend()\nax.grid(True)\n\ntime_text = ax.text(0.02, 0.95, '', transform=ax.transAxes, fontsize=12,\n                    verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\ndef init():\n    line_pinn.set_data([], [])\n    time_text.set_text('')\n    return line_pinn, time_text\n\ndef animate(frame):\n    line_pinn.set_data(x_test, u_pred_grid[frame, :])\n    time_text.set_text(f't = {t_test[frame]:.3f}')\n    return line_pinn, time_text\n\nani = FuncAnimation(fig, animate, init_func=init, frames=N_t, \n                   interval=50, blit=True, repeat=True)\n\nplt.close()  # Prevent duplicate display\nHTML(ani.to_jshtml())"
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": "## Summary\n\nThis notebook demonstrates how to solve the 1D heat equation using a Physics-Informed Neural Network (PINN) implemented in PyTorch. The key features are:\n\n1. **Problem Parameters** (matching the finite difference implementation):\n   - Domain: x ∈ [0, 1], t ∈ [0, 10]\n   - Thermal diffusivity: α = 0.01\n   - Initial condition: u(x, 0) = sin(πx/L)\n   - Boundary conditions: u(0, t) = 0, u(L, t) = 1\n\n2. **Neural Network**: A fully connected network with tanh activation functions [2, 32, 32, 32, 1]\n\n3. **Physics-Informed Loss**: Combination of:\n   - Data loss (initial/boundary conditions)\n   - PDE residual loss (enforces ∂u/∂t = α ∂²u/∂x²)\n\n4. **Automatic Differentiation**: PyTorch's autograd computes the required derivatives for the PDE\n\n5. **Training**: Adam optimizer with 10,000 epochs minimizes the combined loss\n\nThe PINN successfully learns the solution to the heat equation without requiring dense sampling of the solution in the interior of the domain, demonstrating the power of incorporating physical laws directly into the learning process. This PyTorch implementation solves the same problem as the finite difference method but using a neural network approach."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}